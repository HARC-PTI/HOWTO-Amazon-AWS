{
    "docs": [
        {
            "location": "/",
            "text": "Pre-built Tools for Amazon Web Services\n\n\nThis repository contains a collection of tools to run serverless computations and parallel batch jobs on AWS.\n\n\nNecessary AWS credentials\n\n\nThe tools in this repository require access to the \nIAM user name\n and password (to log into the AWS console), as well as to the \nAWS Access Key ID\n and the \nAWS Secret Access key\n.\n\n\nInstall and configure AWS Command Line Interface\n\n\nThe first required step is the installation of the AWS command line interface (CLI). With \npip3\n, the CLI can be installed from a Linux/Unix terminal by executing the following command:\n\n\npip3 install awscli --upgrade --user\n\n\n\n\nOnce installed, the CLI must be configured with the AWS user credentials. Run \naws configure\n from the command line and enter your AWS Access Key ID, the AWS Secret Access Key and a region name (e.g. \nus-east-1\n).\n\n\nClone github repository\n\n\nNext, clone the Github repository that contains all setup scripts and the numerical examples. Here, we add the repository to the home directory (\n~/\n):\n\n\ngit clone https://github.com/HARC-PTI/Pre_Built_Tools-for-Amazon-AWS ~/.\n\n\n\n\nContributing authors\n\n\n\n\nPhilipp Witte, Georgia Institute of Technology. All content copyrighted by the Georgia Institute of Technology, 2019.",
            "title": "Home"
        },
        {
            "location": "/#pre-built-tools-for-amazon-web-services",
            "text": "This repository contains a collection of tools to run serverless computations and parallel batch jobs on AWS.",
            "title": "Pre-built Tools for Amazon Web Services"
        },
        {
            "location": "/#necessary-aws-credentials",
            "text": "The tools in this repository require access to the  IAM user name  and password (to log into the AWS console), as well as to the  AWS Access Key ID  and the  AWS Secret Access key .",
            "title": "Necessary AWS credentials"
        },
        {
            "location": "/#install-and-configure-aws-command-line-interface",
            "text": "The first required step is the installation of the AWS command line interface (CLI). With  pip3 , the CLI can be installed from a Linux/Unix terminal by executing the following command:  pip3 install awscli --upgrade --user  Once installed, the CLI must be configured with the AWS user credentials. Run  aws configure  from the command line and enter your AWS Access Key ID, the AWS Secret Access Key and a region name (e.g.  us-east-1 ).",
            "title": "Install and configure AWS Command Line Interface"
        },
        {
            "location": "/#clone-github-repository",
            "text": "Next, clone the Github repository that contains all setup scripts and the numerical examples. Here, we add the repository to the home directory ( ~/ ):  git clone https://github.com/HARC-PTI/Pre_Built_Tools-for-Amazon-AWS ~/.",
            "title": "Clone github repository"
        },
        {
            "location": "/#contributing-authors",
            "text": "Philipp Witte, Georgia Institute of Technology. All content copyrighted by the Georgia Institute of Technology, 2019.",
            "title": "Contributing authors"
        },
        {
            "location": "/batch/",
            "text": "Batch environment\n\n\nThis tutorial demonstrates how to run multi-node batch jobs on Amazon Web Services. Standard AWS Batch allows processing of embarassingly parallel workloads using containerization. Jobs are processed from a batch queue in parallel as individual docker containers. In the standard setting, each container runs on a single EC2 instance and no communication between jobs and/or workers is possible. Multi-node Batch jobs are an extension of concept, in which it is possible to use multiple EC2 instances per job, where instances communicate via the network.\n\n\nTo run AWS Batch jobs, we need to first set up \nCompute environments\n and \nJob queues\n. The compute environments essentially specify the virtual cluster that AWS Batch has access to. This involves specifying which type of instances are allowed, as well as their size (i.e. the number of available CPUs and memory). For multi-node AWS Batch jobs, we also have to set up a shared file system and a customized Amazon Machine Image (AMI).\n\n\nPrerequisites\n\n\nBefore you can proceed, make sure you have all necessary AWS user and service roles in place. Follow this link to the documentation for creating all necessary user roles.\n\n\nElastic file system\n\n\nFor multi-node AWS Batch jobs, we need to set up a shared file system called \nelastic file system\n (EFS). Furthermore, we need to set up a customized Amazon Machine Image (AMI) and mount the shared file system. Detailed instructions for these steps are provided in the AWS documentation: \nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_efs.html\n. Here, we provide a summary of the necessary steps:\n\n\n1) Create an elastic file system by logging into the AWS console in the web browser and go to \nServices\n -> \nEFS\n -> \nCreate file system\n. By default, AWS will fill in all available subnets and include the default security group. For each zone, also add the SSH-security group. Proceed to step 2 and 3 and then select \nCreate File System\n.\n\n\n2) Next, we have to modify the AMI that is used by AWS Batch and mount the file system. For this, we launch an EC2 instances with the ECS-optimized AMI, mount the EFS and create a custom AMI, which will then be used in the compute environment.\n\n\nChoose the Amazon Linux 2 AMI for your region from the following list:\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html\n. For example, for the \nus-east-1\n region, this is AMI ID \nami-0fac5486e4cff37f4\n. Click on \nLaunch Instance\n to request an EC2 instance with the corresponding AMI. Using the \nt2.micro\n instance type is sufficient for this task. Next, connect to your instance via \nssh\n:\n\n\nssh -Y -i ~/.ssh/user_key_pair -o StrictHostKeyChecking=no -l ec2-user public_DNS_of_instance\n\n\n\n\nOnce you are logged into the instance, following the subsequent steps:\n\n\n\n\n\n\nCreate mount point: \nsudo mkdir /efs\n\n\n\n\n\n\nInstall the amazon-efs-utils client software: \nsudo yum install -y amazon-efs-utils\n\n\n\n\n\n\nMake a backup of the \n/etc/fstab\n file: \nsudo cp /etc/fstab /etc/fstab.bak\n\n\n\n\n\n\nOpen the original file with \nsudo vi /etc/fstab\n and add the following line to it. Replace \nefs_dns_name\n with the DNS name of your elastic file system (find the DNS name in the AWS console -> \nServices\n -> \nEFS\n -> \nname-of-your-file-system\n -> \nDNS name\n):\n\n\n\n\n\n\nefs_dns_name:/ /efs nfs4 nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0\n\n\n\n\n\n\n\n\n\nReload file system: \nsudo mount -a\n\n\n\n\n\n\nValidate that file system is mounted correctly: \nmount | grep efs\n\n\n\n\n\n\nLog out from the instance and create a new AMI from the running EC2 instance. Go the list of running EC2 instances in the AWS console and select your running instance -> \nActions\n -> \nImage\n -> \nCreate Image\n. Choose an image name and then hit \nCreate Image\n.\n\n\nAMIs without hyper-threading\n\n\nBy default, AWS Batch uses hyperthreading (HT) on the underlying EC2 instances. For certain applications, it is desirable to disable HT and to limit the number of cores to half the number of virtual CPU cores on the corresponding EC2 instance. For example, the \nr5.24xlarge\n instance has 96 virtual CPUs and therefore 48 physical cores. To disable HT for this instance, we need to set the maximum number of allowed CPUs to 48.\n\n\nTo disable HT, we modify the AMI that is used by AWS Batch. For this, we launch an EC2 instances with the ECS-optimized AMI, specify the maximum number of allowed CPUs and create a custom AMI. This AMI will then be used in the compute environment.\n\n\nIf you already created an AMI in the previous section with an elastic file system, start a new EC2 instance using this AMI and connect to your instance. If you have not created an AMI yet, choose the Amazon Linux 2 AMI for your region from the following list:\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html\n. For example, for the \nus-east-1\n region, this is AMI ID \nami-0fac5486e4cff37f4\n. Click on \nLaunch Instance\n to request an EC2 instance with the corresponding AMI. Using the \nt2.micro\n instance type is sufficient for this task. Next, connect to your instance via \nssh\n:\n\n\nssh -Y -i ~/.ssh/user_key_pair -o StrictHostKeyChecking=no -l ec2-user public_DNS_of_instance\n\n\n\n\nOpen the grub config file with \nsudo vi /etc/default/grub\n and add \nnr_cpus=48\n to the line starting with \nGRUB_CMDLINE_LINUX\n (or however many cores are required). Apply the changes by running:\n\n\nsudo grub2-mkconfig -o /boot/grub2/grub.cfg\n\n\n\n\nLog out from the instance and create a new AMI from the running EC2 instance. Go the list of running EC2 instances in the AWS console and select your running instance -> \nActions\n -> \nImage\n -> \nCreate Image\n. Choose an image name that indicates the maximum number of cores and then hit \nCreate Image\n.\n\n\nFollow the same steps to create customized AMIs for other instance types with a differet number of CPU cores. E.g. for the \nr5.12xlarge\n instance, set \nnr_cpus=24\n, as this instance type has 48 vCPUs with 24 physicsal cores. For the \nc5n.18xlarge\n instance (72 vCPUs), set \nnr_cpus=36\n and so on.\n\n\nCreate environments\n\n\nCompute environments define the type of instances and the number of cores that are available for AWS Batch jobs. In this example, we set up a compute environment with \nr5\n instances, which are Amazon's memory optimized instances, but the instructions can be modified to include other instance types.\n\n\nThe compute environment can be set up from the command line, with all parameters being specified in the \n~/Pre_Built_Tools-for-Amazon-AWS/How-To/multi-node-batch-jobs/setup/create_environment_r5_multinode_24.json\n file. Open the file and fill in \nall missing entries\n. These are:\n\n\n\n\n\n\nspotIamFleetRole\n: Go to the AWS console -> \nServices\n -> \nIAM\n -> \nRoles\n and find the \nAmazonEC2SpotFleetRole\n. Copy the role ARN and paste it.\n\n\n\n\n\n\nsubnets\n:  Find your subnets in the AWS console at \nServices\n -> \nVPC\n -> \nSubnets\n. Copy the Subnet ID of each subnet into the parameter file (separated by commas).\n\n\n\n\n\n\nsecurityGroupIds\n: Find the security groups in the console at \nServices\n -> \nEC2\n -> \nSecurity groups\n. Copy-paste the Group ID of the default security group. To enable ssh access to instances of AWS Batch jobs, optionally create and add an SSH security group to this list.\n\n\n\n\n\n\nec2KeyPair\n: To connect to running instances via ssh, add the name of your AWS ssh key pair.\n\n\n\n\n\n\nimageId\n: Go to the console -> \nServices\n -> \nEC2\n -> \nAMIs\n and find the AMI that was created in the previous step. For the \nM4_SPOT_MAXCPU_8\n compute environment, find the AMI with 8 cores and copy-paste the AMI-ID into the parameter file.\n\n\n\n\n\n\ninstanceRole\n: Go to the AWS console -> \nServices\n -> \nIAM\n -> \nRoles\n. Find the \nService-Role_ECS_for_EC2\n role and add its ARN to the parameter file.\n\n\n\n\n\n\nserviceRole\n: Go to the AWS console -> \nServices\n -> \nIAM\n -> \nRoles\n. Find the \nAWSBatchServiceRole\n role and add its ARN.\n\n\n\n\n\n\nThe parameter files also specify a placement group called \nMPIgroup\n. The placement group ensures that EC2 instances of the MPI clusters are in close physical vicinity to each other. Create the \nMPIgroup\n placement group from the AWS console \nServices\n -> \nEC2\n -> \nPlacement Groups\n -> \nCreate Placement Group\n. Enter the name \nMPIgroup\n and select \nCluster\n in the \nStrategy\n field. Then click the \ncreate\n button.\n\n\nDo not modify the parameters that are already filled in. Save the updated file and then run the following command within the \n~/Pre_Built_Tools-for-Amazon-AWS/How-To/multi-node-batch-jobs\n directory:\n\n\n# Create environment\naws batch create-compute-environment --cli-input-json file://setup/create_environment_r5_multinode_24.json\n\n\n\n\nYou can go to the AWS Console in the web browser and move to \nServices\n -> \nAWS Batch\n -> \nCompute environments\n to verify that the environment has been created successfully.\n\n\nCreate queues\n\n\nFor each compute environment, we need to create an AWS Batch Job queue, to which jobs can be submitted. The queue parameter files do not need to be modified, so simply run the following commands from the terminal within the \n~/Pre_Built_Tools-for-Amazon-AWS/How-To/multi-node-batch-jobs\n directory:\n\n\n# Job queue r5 on-demand\naws batch create-job-queue --cli-input-json file://setup/create_queue_r5_multinode_24.json\n\n\n\n\n\nVPC endpoints\n\n\nFor multi-node batch jobs, follow these steps to create an endpoint for S3 in your virtual privat cloud:\n\n\n\n\n\n\nLog into the AWS console in the browser and go to: \nServices\n -> \nVPC\n -> \nEndpoints\n.\n\n\n\n\n\n\nCreate an S3 endpoint: click \nCreate Endpoint\n and select the S3 service name from the list, e.g. \ncom.amazonaws.us-east-1.s3\n. Next, select the only available route table in the section \nConfigure route tables\n. Finalize the endpoint by clicking the \nCreate Endpoint\n button.\n\n\n\n\n\n\nDocker\n\n\nAWS Batch runs jobs inside Docker containers and for multi-node batch jobs, the containers need to set up the MPI environment. Once a multi-node batch jobs starts, the containers must establish a network connection via ssh, before being able to run the application. The Dockerfile in \nHow-To/multi-node-batch-jobs/src/Dockerfile\n can be used as a template to build custom Docker container for any application. The container is based on an Ubuntu image and contains all necessary components to establish the MPI environment during runtime.\n\n\nTo create a Docker container for multi-node batch jobs from the Dockerfile, run the following command, using a valid tag for the Docker image (e.g. \nv1.1\n):\n\n\ncd ~/Pre_Built_Tools-for-Amazon-AWS/How-To/multi-node-batch-jobs/src\ndocker build -t aws_multi_node_batch:tag .\n\n\n\n\nYou can upload the Docker image to your personal Docker hub or to your personal AWS account. For the latter case, you first need to obtain login credentials by AWS. Copy-paste the output of the following command back into the terminal:\n\n\n# Get ECR login credentials and use the token that is printed in the terminal\naws ecr get-login --no-include-email\n\n\n\n\nIf the log in was successful, you will see the message \"Login Succeeded\" in your terminal. Next, create a repository on AWS called \naws_multi_node_batch\n:\n\n\n# Create repository (only first time)\naws ecr create-repository --repository-name aws_multi_node_batch\n\n\n\n\nNow, tag the new image using the URI of the repository that you just created. To find the URI of your repository, go to the AWS console -> \nServices\n -> \nECR\n -> \naws_multi_node_batch\n. Tag your image by running:\n\n\n# tag image\ndocker tag aws_multi_node_batch:tag URI:tag\n\n\n\n\nFinally, upload your Docker image to your AWS container registry:\n\n\ndocker push URI:tag\n\n\n\n\nNote the full name of your new image (\nURI:tag\n) for the subsequent steps.\n\n\nSubmitting a multi-node batch job\n\n\nOnce you successfully created and uploaded your Docker container and completed all previous steps, you can finally submit a multi-node AWS Batch job. There are several possibilities for submitting jobs:\n\n\n\n\n\n\nCreate and submit jobs from the AWS console in the web browser\n\n\n\n\n\n\nCreate and submit jobs from the command line using the AWS CLI\n\n\n\n\n\n\nCreate and submit jobs from Python using the \nboto3\n package.\n\n\n\n\n\n\nHere, we will walk you through the third option, as it is the easiest option to reproduce. The file \nHow-To/multi-node-batch-jobs/example/create_and_submit_job.py\n contains a template to you can modify for your purposes. The file contains the following steps:\n\n\n\n\n\n\nDefinition of job parameters. Fill in all missing entries in the script.\n\n\n\n\n\n\nCreating a job definition. This is essentially a job template and specifies the number of nodes per job, as well as the environment variables. You don not need to modify this part.\n\n\n\n\n\n\nSubmitting the job. Using the job definition from the previous step, you submit the AWS Batch job.\n\n\n\n\n\n\nBefore running the script, you have to upload our example application \nmpi_example.py\n (under \nHow-To/multi-node-batch-jobs/example/\n) to an S3 bucket. Note the name of the bucket and the S3 path to your script and fill in the corresponding entries into the \ncreate_and_submit_job.py\n script. Running the script will add the job to the AWS Batch queue. You can go to the console in the webbrowser and check the status of your job under \nAWS Batch\n -> \nJobs\n -> \nMultiNodeQueue_R5_MAX_24\n -> \nRunnable\n. Click on the job ID and go to the \nNodes\n tab. Once your job is scheduled, it will appear under \nStarting\n.",
            "title": "Batch"
        },
        {
            "location": "/batch/#batch-environment",
            "text": "This tutorial demonstrates how to run multi-node batch jobs on Amazon Web Services. Standard AWS Batch allows processing of embarassingly parallel workloads using containerization. Jobs are processed from a batch queue in parallel as individual docker containers. In the standard setting, each container runs on a single EC2 instance and no communication between jobs and/or workers is possible. Multi-node Batch jobs are an extension of concept, in which it is possible to use multiple EC2 instances per job, where instances communicate via the network.  To run AWS Batch jobs, we need to first set up  Compute environments  and  Job queues . The compute environments essentially specify the virtual cluster that AWS Batch has access to. This involves specifying which type of instances are allowed, as well as their size (i.e. the number of available CPUs and memory). For multi-node AWS Batch jobs, we also have to set up a shared file system and a customized Amazon Machine Image (AMI).",
            "title": "Batch environment"
        },
        {
            "location": "/batch/#prerequisites",
            "text": "Before you can proceed, make sure you have all necessary AWS user and service roles in place. Follow this link to the documentation for creating all necessary user roles.",
            "title": "Prerequisites"
        },
        {
            "location": "/batch/#elastic-file-system",
            "text": "For multi-node AWS Batch jobs, we need to set up a shared file system called  elastic file system  (EFS). Furthermore, we need to set up a customized Amazon Machine Image (AMI) and mount the shared file system. Detailed instructions for these steps are provided in the AWS documentation:  https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_efs.html . Here, we provide a summary of the necessary steps:  1) Create an elastic file system by logging into the AWS console in the web browser and go to  Services  ->  EFS  ->  Create file system . By default, AWS will fill in all available subnets and include the default security group. For each zone, also add the SSH-security group. Proceed to step 2 and 3 and then select  Create File System .  2) Next, we have to modify the AMI that is used by AWS Batch and mount the file system. For this, we launch an EC2 instances with the ECS-optimized AMI, mount the EFS and create a custom AMI, which will then be used in the compute environment.  Choose the Amazon Linux 2 AMI for your region from the following list: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html . For example, for the  us-east-1  region, this is AMI ID  ami-0fac5486e4cff37f4 . Click on  Launch Instance  to request an EC2 instance with the corresponding AMI. Using the  t2.micro  instance type is sufficient for this task. Next, connect to your instance via  ssh :  ssh -Y -i ~/.ssh/user_key_pair -o StrictHostKeyChecking=no -l ec2-user public_DNS_of_instance  Once you are logged into the instance, following the subsequent steps:    Create mount point:  sudo mkdir /efs    Install the amazon-efs-utils client software:  sudo yum install -y amazon-efs-utils    Make a backup of the  /etc/fstab  file:  sudo cp /etc/fstab /etc/fstab.bak    Open the original file with  sudo vi /etc/fstab  and add the following line to it. Replace  efs_dns_name  with the DNS name of your elastic file system (find the DNS name in the AWS console ->  Services  ->  EFS  ->  name-of-your-file-system  ->  DNS name ):    efs_dns_name:/ /efs nfs4 nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0    Reload file system:  sudo mount -a    Validate that file system is mounted correctly:  mount | grep efs    Log out from the instance and create a new AMI from the running EC2 instance. Go the list of running EC2 instances in the AWS console and select your running instance ->  Actions  ->  Image  ->  Create Image . Choose an image name and then hit  Create Image .",
            "title": "Elastic file system"
        },
        {
            "location": "/batch/#amis-without-hyper-threading",
            "text": "By default, AWS Batch uses hyperthreading (HT) on the underlying EC2 instances. For certain applications, it is desirable to disable HT and to limit the number of cores to half the number of virtual CPU cores on the corresponding EC2 instance. For example, the  r5.24xlarge  instance has 96 virtual CPUs and therefore 48 physical cores. To disable HT for this instance, we need to set the maximum number of allowed CPUs to 48.  To disable HT, we modify the AMI that is used by AWS Batch. For this, we launch an EC2 instances with the ECS-optimized AMI, specify the maximum number of allowed CPUs and create a custom AMI. This AMI will then be used in the compute environment.  If you already created an AMI in the previous section with an elastic file system, start a new EC2 instance using this AMI and connect to your instance. If you have not created an AMI yet, choose the Amazon Linux 2 AMI for your region from the following list: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html . For example, for the  us-east-1  region, this is AMI ID  ami-0fac5486e4cff37f4 . Click on  Launch Instance  to request an EC2 instance with the corresponding AMI. Using the  t2.micro  instance type is sufficient for this task. Next, connect to your instance via  ssh :  ssh -Y -i ~/.ssh/user_key_pair -o StrictHostKeyChecking=no -l ec2-user public_DNS_of_instance  Open the grub config file with  sudo vi /etc/default/grub  and add  nr_cpus=48  to the line starting with  GRUB_CMDLINE_LINUX  (or however many cores are required). Apply the changes by running:  sudo grub2-mkconfig -o /boot/grub2/grub.cfg  Log out from the instance and create a new AMI from the running EC2 instance. Go the list of running EC2 instances in the AWS console and select your running instance ->  Actions  ->  Image  ->  Create Image . Choose an image name that indicates the maximum number of cores and then hit  Create Image .  Follow the same steps to create customized AMIs for other instance types with a differet number of CPU cores. E.g. for the  r5.12xlarge  instance, set  nr_cpus=24 , as this instance type has 48 vCPUs with 24 physicsal cores. For the  c5n.18xlarge  instance (72 vCPUs), set  nr_cpus=36  and so on.",
            "title": "AMIs without hyper-threading"
        },
        {
            "location": "/batch/#create-environments",
            "text": "Compute environments define the type of instances and the number of cores that are available for AWS Batch jobs. In this example, we set up a compute environment with  r5  instances, which are Amazon's memory optimized instances, but the instructions can be modified to include other instance types.  The compute environment can be set up from the command line, with all parameters being specified in the  ~/Pre_Built_Tools-for-Amazon-AWS/How-To/multi-node-batch-jobs/setup/create_environment_r5_multinode_24.json  file. Open the file and fill in  all missing entries . These are:    spotIamFleetRole : Go to the AWS console ->  Services  ->  IAM  ->  Roles  and find the  AmazonEC2SpotFleetRole . Copy the role ARN and paste it.    subnets :  Find your subnets in the AWS console at  Services  ->  VPC  ->  Subnets . Copy the Subnet ID of each subnet into the parameter file (separated by commas).    securityGroupIds : Find the security groups in the console at  Services  ->  EC2  ->  Security groups . Copy-paste the Group ID of the default security group. To enable ssh access to instances of AWS Batch jobs, optionally create and add an SSH security group to this list.    ec2KeyPair : To connect to running instances via ssh, add the name of your AWS ssh key pair.    imageId : Go to the console ->  Services  ->  EC2  ->  AMIs  and find the AMI that was created in the previous step. For the  M4_SPOT_MAXCPU_8  compute environment, find the AMI with 8 cores and copy-paste the AMI-ID into the parameter file.    instanceRole : Go to the AWS console ->  Services  ->  IAM  ->  Roles . Find the  Service-Role_ECS_for_EC2  role and add its ARN to the parameter file.    serviceRole : Go to the AWS console ->  Services  ->  IAM  ->  Roles . Find the  AWSBatchServiceRole  role and add its ARN.    The parameter files also specify a placement group called  MPIgroup . The placement group ensures that EC2 instances of the MPI clusters are in close physical vicinity to each other. Create the  MPIgroup  placement group from the AWS console  Services  ->  EC2  ->  Placement Groups  ->  Create Placement Group . Enter the name  MPIgroup  and select  Cluster  in the  Strategy  field. Then click the  create  button.  Do not modify the parameters that are already filled in. Save the updated file and then run the following command within the  ~/Pre_Built_Tools-for-Amazon-AWS/How-To/multi-node-batch-jobs  directory:  # Create environment\naws batch create-compute-environment --cli-input-json file://setup/create_environment_r5_multinode_24.json  You can go to the AWS Console in the web browser and move to  Services  ->  AWS Batch  ->  Compute environments  to verify that the environment has been created successfully.",
            "title": "Create environments"
        },
        {
            "location": "/batch/#create-queues",
            "text": "For each compute environment, we need to create an AWS Batch Job queue, to which jobs can be submitted. The queue parameter files do not need to be modified, so simply run the following commands from the terminal within the  ~/Pre_Built_Tools-for-Amazon-AWS/How-To/multi-node-batch-jobs  directory:  # Job queue r5 on-demand\naws batch create-job-queue --cli-input-json file://setup/create_queue_r5_multinode_24.json",
            "title": "Create queues"
        },
        {
            "location": "/batch/#vpc-endpoints",
            "text": "For multi-node batch jobs, follow these steps to create an endpoint for S3 in your virtual privat cloud:    Log into the AWS console in the browser and go to:  Services  ->  VPC  ->  Endpoints .    Create an S3 endpoint: click  Create Endpoint  and select the S3 service name from the list, e.g.  com.amazonaws.us-east-1.s3 . Next, select the only available route table in the section  Configure route tables . Finalize the endpoint by clicking the  Create Endpoint  button.",
            "title": "VPC endpoints"
        },
        {
            "location": "/batch/#docker",
            "text": "AWS Batch runs jobs inside Docker containers and for multi-node batch jobs, the containers need to set up the MPI environment. Once a multi-node batch jobs starts, the containers must establish a network connection via ssh, before being able to run the application. The Dockerfile in  How-To/multi-node-batch-jobs/src/Dockerfile  can be used as a template to build custom Docker container for any application. The container is based on an Ubuntu image and contains all necessary components to establish the MPI environment during runtime.  To create a Docker container for multi-node batch jobs from the Dockerfile, run the following command, using a valid tag for the Docker image (e.g.  v1.1 ):  cd ~/Pre_Built_Tools-for-Amazon-AWS/How-To/multi-node-batch-jobs/src\ndocker build -t aws_multi_node_batch:tag .  You can upload the Docker image to your personal Docker hub or to your personal AWS account. For the latter case, you first need to obtain login credentials by AWS. Copy-paste the output of the following command back into the terminal:  # Get ECR login credentials and use the token that is printed in the terminal\naws ecr get-login --no-include-email  If the log in was successful, you will see the message \"Login Succeeded\" in your terminal. Next, create a repository on AWS called  aws_multi_node_batch :  # Create repository (only first time)\naws ecr create-repository --repository-name aws_multi_node_batch  Now, tag the new image using the URI of the repository that you just created. To find the URI of your repository, go to the AWS console ->  Services  ->  ECR  ->  aws_multi_node_batch . Tag your image by running:  # tag image\ndocker tag aws_multi_node_batch:tag URI:tag  Finally, upload your Docker image to your AWS container registry:  docker push URI:tag  Note the full name of your new image ( URI:tag ) for the subsequent steps.",
            "title": "Docker"
        },
        {
            "location": "/batch/#submitting-a-multi-node-batch-job",
            "text": "Once you successfully created and uploaded your Docker container and completed all previous steps, you can finally submit a multi-node AWS Batch job. There are several possibilities for submitting jobs:    Create and submit jobs from the AWS console in the web browser    Create and submit jobs from the command line using the AWS CLI    Create and submit jobs from Python using the  boto3  package.    Here, we will walk you through the third option, as it is the easiest option to reproduce. The file  How-To/multi-node-batch-jobs/example/create_and_submit_job.py  contains a template to you can modify for your purposes. The file contains the following steps:    Definition of job parameters. Fill in all missing entries in the script.    Creating a job definition. This is essentially a job template and specifies the number of nodes per job, as well as the environment variables. You don not need to modify this part.    Submitting the job. Using the job definition from the previous step, you submit the AWS Batch job.    Before running the script, you have to upload our example application  mpi_example.py  (under  How-To/multi-node-batch-jobs/example/ ) to an S3 bucket. Note the name of the bucket and the S3 path to your script and fill in the corresponding entries into the  create_and_submit_job.py  script. Running the script will add the job to the AWS Batch queue. You can go to the console in the webbrowser and check the status of your job under  AWS Batch  ->  Jobs  ->  MultiNodeQueue_R5_MAX_24  ->  Runnable . Click on the job ID and go to the  Nodes  tab. Once your job is scheduled, it will appear under  Starting .",
            "title": "Submitting a multi-node batch job"
        },
        {
            "location": "/roles/",
            "text": "IAM roles\n\n\nAWS manages access to its services through \nroles\n. Users and AWS services such as Lambda functions require explicit permissions to interact with other services or to request computational resources. \nUser roles\n provide permissions to a specific IAM user, while \nservice roles\n allow specific AWS services to interact with each other. For example, to start an AWS Batch job from the command line, users require the \nAWSBatchFullAccess\n user role. If we want to allow a container launched by AWS Batch to send messages to an SQS queue, we need to provide an AWS service role for AWS Batch and attach the \nAmazonSQSFullAccess\n policy to it. The following instructions create the necessary user and service roles for our workflow.\n\n\nUser roles\n\n\nLog into the AWS console (\nhttps://console.aws.amazon.com/console\n) and check if the following roles are attached to your user in \nServices\n -> \nIAM\n -> \nUsers\n -> \nyour_user_name\n. Run the following commands in a terminal to obtain the missing permissions that are not attached to your account so far. (Replace \nyour_user_name\n by your IAM user name).\n\n\n# EC2\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonEC2FullAccess\n\n# Batch\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AWSBatchFullAccess\n\n# ECR\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess\n\n# Lambda\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AWSLambdaFullAccess     \n\n# SQS\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonSQSFullAccess\n\n# Step Functions\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AWSStepFunctionsFullAccess\n\n# S3 Read\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\n\n\n\n\nService roles\n\n\nThe following service roles are required for the workflow:\n\n\n\n\nAWSBatchServiceRole\n\n\n\n\nCheck if the role exists in your AWS console under \nIAM\n -> \nRoles\n. If not, open a terminal in the current directory (\n~/Pre_Built_Tools-for-Amazon-AWS/How-To/create-roles\n) and run the following commands:\n\n\n# Create role\naws iam create-role --role-name AWSBatchServiceRole  \\\n    --assume-role-policy-document file://src/create_AWSBatchServiceRole.json \\\n    --description \"Allows Batch to create and manage AWS resources on your behalf.\"\n\n# Attach policy\naws iam attach-role-policy --role-name AWSBatchServiceRole --policy-arn \\\n    arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole\n\n\n\n\n\n\nStepFunctionsLambdaRole\n\n\n\n\nCheck if the role exists in \nIAM\n -> \nRoles\n. If not, open a terminal in the current directory (\n~/cloud-imaging\n) and run the following commands:\n\n\n# Create role\naws iam create-role --role-name StepFunctionsLambdaRole  \\\n    --assume-role-policy-document file://src/create_StepFunctionsLambdaRole.json \\\n    --description \"Allows Step Functions to access AWS resources on your behalf.\"\n\n# Attach policy\naws iam attach-role-policy --role-name StepFunctionsLambdaRole --policy-arn \\\n    arn:aws:iam::aws:policy/service-role/AWSLambdaRole\n\n\n\n\n\n\nService-Role_ECS_for_EC2\n\n\n\n\nCreate this role, regardless of whether any other ECS roles exist so far:\n\n\n# Create role\naws iam create-role --role-name Service-Role_ECS_for_EC2  \\\n    --assume-role-policy-document file://src/create_Service-Role_ECS_for_EC2.json \\\n    --description \"Allows EC2 instances in an ECS cluster to access ECS.\"\n\n# Attach policies\naws iam attach-role-policy --role-name Service-Role_ECS_for_EC2 --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonSQSFullAccess\n\naws iam attach-role-policy --role-name Service-Role_ECS_for_EC2 --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonS3FullAccess\n\naws iam attach-role-policy --role-name Service-Role_ECS_for_EC2 --policy-arn \\\n    arn:aws:iam::aws:policy/IAMReadOnlyAccess\n\naws iam attach-role-policy --role-name Service-Role_ECS_for_EC2 --policy-arn \\\n    arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\n\n\n\n\n\n\nService-Role_for_Lambda\n\n\n\n\nCreate this role, regardless of whether any other Lambda roles exist so far:\n\n\n# Create role\naws iam create-role --role-name Service-Role_for_Lambda \\\n    --assume-role-policy-document file://src/create_Service-Role_for_Lambda.json \\\n    --description \"Allows Lambda functions to call AWS services on your behalf.\"\n\n# Attach policies\naws iam attach-role-policy --role-name Service-Role_for_Lambda --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonSQSFullAccess\n\naws iam attach-role-policy --role-name Service-Role_for_Lambda --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonS3FullAccess\n\naws iam attach-role-policy --role-name Service-Role_for_Lambda --policy-arn \\\n    arn:aws:iam::aws:policy/AWSLambdaFullAccess\n\naws iam attach-role-policy --role-name Service-Role_for_Lambda --policy-arn \\\n    arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole\n\n# Create and attach specialized Lambda for Batch policy\naws iam create-policy --policy-name LambdaBatchExecutionPolicy \\\n    --policy-document file://src/create_LambdaBatchExecutionPolicy.json\\\n    --description \"Allow Lambda to access AWS Batch services including job registration and submission.\"\n\naws iam attach-role-policy --role-name Service-Role_for_Lambda \\\n    --policy-arn arn:aws:iam::851065145468:policy/LambdaBatchExecutionPolicy\n\n\n\n\n\n\nRoles for using Spot instances with Batch\n\n\n\n\nCreate the following roles to enable spot instances for usage with Batch:\n\n\n# AmazonEC2SpotFleetRole\naws iam create-role --role-name AmazonEC2SpotFleetRole --assume-role-policy-document '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"spotfleet.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}'\n\n# AWSServiceRoleForEC2Spot\naws iam create-service-linked-role --aws-service-name spot.amazonaws.com\n\n# AWSServiceRoleForEC2SpotFleet\naws iam create-service-linked-role --aws-service-name spotfleet.amazonaws.com",
            "title": "Roles"
        },
        {
            "location": "/roles/#iam-roles",
            "text": "AWS manages access to its services through  roles . Users and AWS services such as Lambda functions require explicit permissions to interact with other services or to request computational resources.  User roles  provide permissions to a specific IAM user, while  service roles  allow specific AWS services to interact with each other. For example, to start an AWS Batch job from the command line, users require the  AWSBatchFullAccess  user role. If we want to allow a container launched by AWS Batch to send messages to an SQS queue, we need to provide an AWS service role for AWS Batch and attach the  AmazonSQSFullAccess  policy to it. The following instructions create the necessary user and service roles for our workflow.",
            "title": "IAM roles"
        },
        {
            "location": "/roles/#user-roles",
            "text": "Log into the AWS console ( https://console.aws.amazon.com/console ) and check if the following roles are attached to your user in  Services  ->  IAM  ->  Users  ->  your_user_name . Run the following commands in a terminal to obtain the missing permissions that are not attached to your account so far. (Replace  your_user_name  by your IAM user name).  # EC2\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonEC2FullAccess\n\n# Batch\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AWSBatchFullAccess\n\n# ECR\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess\n\n# Lambda\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AWSLambdaFullAccess     \n\n# SQS\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonSQSFullAccess\n\n# Step Functions\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AWSStepFunctionsFullAccess\n\n# S3 Read\naws iam attach-user-policy --user-name your_user_name --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess",
            "title": "User roles"
        },
        {
            "location": "/roles/#service-roles",
            "text": "The following service roles are required for the workflow:   AWSBatchServiceRole   Check if the role exists in your AWS console under  IAM  ->  Roles . If not, open a terminal in the current directory ( ~/Pre_Built_Tools-for-Amazon-AWS/How-To/create-roles ) and run the following commands:  # Create role\naws iam create-role --role-name AWSBatchServiceRole  \\\n    --assume-role-policy-document file://src/create_AWSBatchServiceRole.json \\\n    --description \"Allows Batch to create and manage AWS resources on your behalf.\"\n\n# Attach policy\naws iam attach-role-policy --role-name AWSBatchServiceRole --policy-arn \\\n    arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole   StepFunctionsLambdaRole   Check if the role exists in  IAM  ->  Roles . If not, open a terminal in the current directory ( ~/cloud-imaging ) and run the following commands:  # Create role\naws iam create-role --role-name StepFunctionsLambdaRole  \\\n    --assume-role-policy-document file://src/create_StepFunctionsLambdaRole.json \\\n    --description \"Allows Step Functions to access AWS resources on your behalf.\"\n\n# Attach policy\naws iam attach-role-policy --role-name StepFunctionsLambdaRole --policy-arn \\\n    arn:aws:iam::aws:policy/service-role/AWSLambdaRole   Service-Role_ECS_for_EC2   Create this role, regardless of whether any other ECS roles exist so far:  # Create role\naws iam create-role --role-name Service-Role_ECS_for_EC2  \\\n    --assume-role-policy-document file://src/create_Service-Role_ECS_for_EC2.json \\\n    --description \"Allows EC2 instances in an ECS cluster to access ECS.\"\n\n# Attach policies\naws iam attach-role-policy --role-name Service-Role_ECS_for_EC2 --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonSQSFullAccess\n\naws iam attach-role-policy --role-name Service-Role_ECS_for_EC2 --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonS3FullAccess\n\naws iam attach-role-policy --role-name Service-Role_ECS_for_EC2 --policy-arn \\\n    arn:aws:iam::aws:policy/IAMReadOnlyAccess\n\naws iam attach-role-policy --role-name Service-Role_ECS_for_EC2 --policy-arn \\\n    arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role   Service-Role_for_Lambda   Create this role, regardless of whether any other Lambda roles exist so far:  # Create role\naws iam create-role --role-name Service-Role_for_Lambda \\\n    --assume-role-policy-document file://src/create_Service-Role_for_Lambda.json \\\n    --description \"Allows Lambda functions to call AWS services on your behalf.\"\n\n# Attach policies\naws iam attach-role-policy --role-name Service-Role_for_Lambda --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonSQSFullAccess\n\naws iam attach-role-policy --role-name Service-Role_for_Lambda --policy-arn \\\n    arn:aws:iam::aws:policy/AmazonS3FullAccess\n\naws iam attach-role-policy --role-name Service-Role_for_Lambda --policy-arn \\\n    arn:aws:iam::aws:policy/AWSLambdaFullAccess\n\naws iam attach-role-policy --role-name Service-Role_for_Lambda --policy-arn \\\n    arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole\n\n# Create and attach specialized Lambda for Batch policy\naws iam create-policy --policy-name LambdaBatchExecutionPolicy \\\n    --policy-document file://src/create_LambdaBatchExecutionPolicy.json\\\n    --description \"Allow Lambda to access AWS Batch services including job registration and submission.\"\n\naws iam attach-role-policy --role-name Service-Role_for_Lambda \\\n    --policy-arn arn:aws:iam::851065145468:policy/LambdaBatchExecutionPolicy   Roles for using Spot instances with Batch   Create the following roles to enable spot instances for usage with Batch:  # AmazonEC2SpotFleetRole\naws iam create-role --role-name AmazonEC2SpotFleetRole --assume-role-policy-document '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"spotfleet.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}'\n\n# AWSServiceRoleForEC2Spot\naws iam create-service-linked-role --aws-service-name spot.amazonaws.com\n\n# AWSServiceRoleForEC2SpotFleet\naws iam create-service-linked-role --aws-service-name spotfleet.amazonaws.com",
            "title": "Service roles"
        },
        {
            "location": "/trouble/",
            "text": "Troubleshooting\n\n\nThe best reference point for questions and troubleshooting is the AWS documentation of the respective services:\n\n\n\n\n\n\nAWS EC2: \nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html\n\n\n\n\n\n\nAWS Batch: \nhttps://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html\n\n\n\n\n\n\nAWS multi-node jobs: \nhttps://docs.aws.amazon.com/batch/latest/userguide/multi-node-parallel-jobs.html\n\n\n\n\n\n\nAWS Lambda: \nhttps://docs.aws.amazon.com/lambda/latest/dg/welcome.html\n\n\n\n\n\n\nAWS S3: \nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html\n\n\n\n\n\n\nAWS Step Functions: \nhttps://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\n\n\n\n\n\n\nAWS SQS: \nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\n\n\n\n\n\n\nFor questions, feel free to contact \nPhilipp Witte\n at \npwitte3@gatech.edu\n.",
            "title": "Trouble"
        },
        {
            "location": "/trouble/#troubleshooting",
            "text": "The best reference point for questions and troubleshooting is the AWS documentation of the respective services:    AWS EC2:  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html    AWS Batch:  https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html    AWS multi-node jobs:  https://docs.aws.amazon.com/batch/latest/userguide/multi-node-parallel-jobs.html    AWS Lambda:  https://docs.aws.amazon.com/lambda/latest/dg/welcome.html    AWS S3:  https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html    AWS Step Functions:  https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html    AWS SQS:  https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html    For questions, feel free to contact  Philipp Witte  at  pwitte3@gatech.edu .",
            "title": "Troubleshooting"
        }
    ]
}